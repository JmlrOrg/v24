{
    "abstract": "High-dimensional spatio-temporal dynamics can often be encoded in a low-dimensional subspace.  Engineering applications for modeling, characterization, design, and control of such large-scale systems often rely on dimensionality reduction to make solutions computationally tractable in real time.  Common existing paradigms for dimensionality reduction include linear methods, such as the singular value decomposition (SVD), and nonlinear methods, such as variants of convolutional autoencoders (CAE). However, these encoding techniques lack the ability to efficiently represent the complexity associated with spatio-temporal data, which often requires variable geometry, non-uniform grid resolution, adaptive meshing, and/or parametric dependencies. To resolve these practical engineering challenges, we propose a general framework called Neural Implicit Flow (NIF) that enables a mesh-agnostic, low-rank representation of large-scale, parametric, spatial-temporal data. NIF consists of two modified multilayer perceptrons (MLPs): (i) ShapeNet, which isolates and represents the spatial complexity, and (ii) ParameterNet, which accounts for any other input complexity, including parametric dependencies, time, and sensor measurements. We demonstrate the utility of NIF for parametric surrogate modeling, enabling the interpretable representation and compression of complex spatio-temporal dynamics, efficient many-spatial-query tasks, and improved generalization performance for sparse reconstruction.",
    "authors": [
        "Shaowu Pan",
        "Steven L. Brunton",
        "J. Nathan Kutz"
    ],
    "emails": [
        "shawnpan@uw.edu",
        "sbrunton@uw.edu",
        "kutz@uw.edu"
    ],
    "extra_links": [
        [
            "code",
            "https://github.com/pswpswpsw/paper-nif"
        ]
    ],
    "id": "22-0365",
    "issue": 41,
    "pages": [
        1,
        60
    ],
    "title": "Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data",
    "volume": 24,
    "year": 2023
}