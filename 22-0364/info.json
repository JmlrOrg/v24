{
    "abstract": "Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions.",
    "authors": [
        "Yoshua Bengio",
        "Salem Lahlou",
        "Tristan Deleu",
        "Edward J. Hu",
        "Mo Tiwari",
        "Emmanuel Bengio"
    ],
    "emails": [
        "yoshua.bengio@mila.quebec",
        "lahlosal@mila.quebec",
        "deleutri@mila.quebec",
        "edward@edwardjhu.com",
        "motiwari@stanford.edu",
        "bengioem@mila.quebec"
    ],
    "id": "22-0364",
    "issue": 210,
    "pages": [
        1,
        55
    ],
    "title": "GFlowNet Foundations",
    "volume": 24,
    "year": 2023
}