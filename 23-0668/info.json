{
    "abstract": "Contrastive adversarial training has successfully improved the robustness of contrastive learning (CL). However, the robustness metric in these methods depends on attack algorithms, image labels, and downstream tasks, introducing reliability concerns. To address these issues, this paper proposes a novel Robustness Verification framework for Contrastive Learning (RVCL). Specifically, we define the verification problem of CL from deterministic and probabilistic perspectives, then provide several effective metrics to evaluate the robustness of CL encoder. Furthermore, we use extreme value theory to reveal the relationship between the robust radius of the CL encoder and that of the supervised downstream task. Extensive experiments on various benchmark models and datasets validate theoretical findings, and further demonstrate RVCL's capability to evaluate the robustness of both CL encoders and images. Our code is available at https://github.com/wzekai99/RVCL-JMLR.",
    "authors": [
        "Zekai Wang",
        "Weiwei Liu"
    ],
    "emails": [
        "wzekai99@gmail.com",
        "liuweiwei863@gmail.com"
    ],
    "extra_links": [
        [
            "code",
            "https://github.com/wzekai99/RVCL-JMLR"
        ]
    ],
    "id": "23-0668",
    "issue": 396,
    "pages": [
        1,
        43
    ],
    "title": "RVCL: Evaluating the Robustness of Contrastive Learning via Verification",
    "volume": 24,
    "year": 2023
}