{
    "abstract": "We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the \u201choly grail\u201d of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be \u2018maximising diversity\u2019 as so many works aim to do---instead, we have a bias/variance/diversity trade-off to manage.",
    "authors": [
        "Danny Wood",
        "Tingting Mu",
        "Andrew M. Webb",
        "Henry W. J. Reeve",
        "Mikel Luj{{\\\'a}}n",
        "Gavin Brown"
    ],
    "emails": [
        "danny.wood@manchester.ac.uk",
        "tingting.mu@manchester.ac.uk",
        "andrew.webb@manchester.ac.uk",
        "henry.reeve@bristol.ac.uk",
        "mikel.lujan@manchester.ac.uk",
        "gavin.brown@manchester.ac.uk"
    ],
    "extra_links": [
        [
            "code",
            "https://github.com/EchoStatements/Decompose"
        ]
    ],
    "id": "23-0041",
    "issue": 359,
    "pages": [
        1,
        49
    ],
    "title": "A Unified Theory of Diversity in Ensemble Learning",
    "volume": 24,
    "year": 2023
}
