{
    "abstract": "Population-based multi-agent reinforcement learning (PB-MARL) encompasses a range of methods that merge dynamic population selection with multi-agent reinforcement learning algorithms (MARL). While PB-MARL has demonstrated notable achievements in complex multi-agent tasks, its sequential execution is plagued by low computational efficiency due to the diversity in computing patterns and policy combinations. We propose a solution involving a stateless central task dispatcher and stateful workers to handle PB-MARL's subroutines, thereby capitalizing on parallelism across various components for efficient problem-solving. In line with this approach, we introduce MALib, a parallel framework that incorporates a task control model, independent data servers, and an abstraction of MARL training paradigms. The framework has undergone extensive testing and is available under the MIT license (https://github.com/sjtu-marl/malib)",
    "authors": [
        "Ming Zhou",
        "Ziyu Wan",
        "Hanjing Wang",
        "Muning Wen",
        "Runzhe Wu",
        "Ying Wen",
        "Yaodong Yang",
        "Yong Yu",
        "Jun Wang",
        "Weinan Zhang"
    ],
    "emails": [
        "mingak@sjtu.edu.cn",
        "alex_wan@sjtu.edu.cn",
        "wanghanjingwhj@sjtu.edu.cn",
        "muning.wen@outlook.com",
        "runzhe@sjtu.edu.cn",
        "ying.wen@sjtu.edu.cn",
        "yaodong.yang@pku.edu.cn",
        "yyu@apex.sjtu.edu.cn",
        "Jun.wang@cs.ucl.ac.uk",
        "wnzhang@sjtu.com"
    ],
    "extra_links": [
        [
            "code",
            "https://github.com/sjtu-marl/malib"
        ]
    ],
    "id": "22-0169",
    "issue": 150,
    "pages": [
        1,
        12
    ],
    "special_issue": "MLOSS",
    "title": "MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning",
    "volume": 24,
    "year": 2023
}